# OpenAI Models
openai:
  model_name: "gpt-4-turbo-preview"
  api_base: "https://api.openai.com/v1"
  temperature: 0.1  # Low for factual legal responses
  max_tokens: 4000
  top_p: 0.9
  frequency_penalty: 0.1
  presence_penalty: 0.0
  timeout: 60

  models:
    gpt_4_turbo:
      model_name: "gpt-4-turbo-preview"
      max_tokens: 4000
      context_window: 128000
      cost_per_1k_input: 0.01
      cost_per_1k_output: 0.03
      best_for: "complex_legal_analysis"

    gpt_4:
      model_name: "gpt-4"
      max_tokens: 4000
      context_window: 8192
      cost_per_1k_input: 0.03
      cost_per_1k_output: 0.06
      best_for: "detailed_reasoning"

    gpt_3_5_turbo:
      model_name: "gpt-3.5-turbo"
      max_tokens: 4000
      context_window: 16385
      cost_per_1k_input: 0.0005
      cost_per_1k_output: 0.0015
      best_for: "quick_responses"

# Anthropic Claude
anthropic:
  model_name: "claude-3-opus-20240229"
  api_base: "https://api.anthropic.com"
  temperature: 0.1
  max_tokens: 4000
  top_p: 0.9

  models:
    opus:
      model_name: "claude-3-opus-20240229"
      max_tokens: 4000
      context_window: 200000
      cost_per_1k_input: 0.015
      cost_per_1k_output: 0.075
      best_for: "complex_legal_reasoning"

    sonnet:
      model_name: "claude-3-sonnet-20240229"
      max_tokens: 4000
      context_window: 200000
      cost_per_1k_input: 0.003
      cost_per_1k_output: 0.015
      best_for: "balanced_performance"

    haiku:
      model_name: "claude-3-haiku-20240307"
      max_tokens: 4000
      context_window: 200000
      cost_per_1k_input: 0.00025
      cost_per_1k_output: 0.00125
      best_for: "fast_responses"

# Local/Open Source Models
huggingface:
  model_name: "microsoft/DialoGPT-large"
  device: "cpu"
  torch_dtype: "float32"
  trust_remote_code: false

  models:
    legal_bert:
      model_name: "nlpaueb/legal-bert-large-uncased"
      specialized_for: "legal_domain"

    llama2_7b:
      model_name: "meta-llama/llama-2-7b-chat-hf"
      requires_auth: true
      device: "cuda"
      torch_dtype: "float16"

# Local server (Ollama, LocalAI, etc.)
local:
  base_url: "http://localhost:11434"
  model_name: "llama2:7b"
  temperature: 0.1
  timeout: 120

  models:
    llama2_7b:
      model_name: "llama2:7b"
      good_for: "general_purpose"

    codellama:
      model_name: "codellama:7b"
      good_for: "code_generation"